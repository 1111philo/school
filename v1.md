# **PRD — 1111 School (Generative Learning Platform)**

## **1\) Overview**

### **Product summary**

**1111 School** is a generative learning platform where users create personalized courses by providing a brief description and learning objectives. The platform uses a set of specialized agents to generate course descriptions, lesson plans, lessons, activities, feedback, and an assessment—personalized to each learner based on an evolving learner profile.

### **Primary outcomes**

* Users can complete **predefined courses** or **user-generated courses** with personalized lessons and activities.  
* The app builds and continuously updates a **Learner Profile** using setup-course signals and ongoing learning behavior.  
* The system provides **transparent traceability** via an **Agent Log** containing prompts and outputs per agent run.

---

## **2\) Goals, Non-Goals, Success Metrics**

### **Goals**

1. Enable fast creation of high-quality, structured courses from minimal input (description \+ objectives).  
2. Personalize lessons/activities/feedback based on learner profile and progress.  
3. Enforce a consistent course structure: **Lessons → Activities → Feedback → Unlock next content → Assessment**.  
4. Provide auditability: users/admins can inspect agent prompts/outputs per step.  
5. Securely store user data and generated content with careful local caching.

### **Non-goals (v1)**

* Real-time collaboration/multi-learner classrooms.  
* Marketplace/payment for courses.  
* External LMS gradebook sync.  
* Full authoring studio with drag/drop lesson editing (basic editing is allowed).

### **Success metrics (v1)**

* Activation: % users completing Setup Course within first session/day.  
* Course completion rate: % started courses completed.  
* Time-to-first-lesson: median time from selecting a course to first lesson displayed.  
* Activity quality: average activity reviewer score and correlation with assessment success.  
* Retention: 7-day return rate after first course start.  
* Personalization perceived value: in-app rating after course completion.

---

## **3\) Personas**

1. **Self-directed learner**: wants quick, structured learning tailored to their needs.  
2. **Professional upskiller**: wants portfolio artifacts and measurable mastery criteria.  
3. **Accessibility-focused learner**: benefits from UDL accommodations and visual supports.

---

## **4\) UX Requirements**

### **4.1 Authentication**

**Entry options**

* Log in using platform credentials (SSO/OAuth) OR create a new user account.  
* Required: email/username, password or SSO token.

**Acceptance criteria**

* User can create account, log in, log out.  
* Session persistence across app restarts (secure tokens).

---

### **4.2 First-time Setup Course (Onboarding)**

**Flow**

1. After first login, user is automatically enrolled into **Setup Course**.  
2. Setup Course teaches:  
   * How lessons/activities/assessment work  
   * How personalization works  
   * What the agent log is  
3. Setup Course collects information used to create initial **Learner Profile**.

**Collected signals (examples, configurable)**

* Learning goals  
* Prior experience level (novice/intermediate/advanced)  
* Preferences (pace, tone, examples)  
* Accessibility needs (UDL preferences, reading level, captions, dyslexia-friendly formatting)  
* Interests/context for examples (industry, hobbies)  
* Preferred response modality (writing, diagrams, image uploads)

**Acceptance criteria**

* Setup Course completion generates a Learner Profile draft.  
* User is directed to Learner Profile review screen after completion.

---

### **4.3 Learner Profile (Editable \+ Continuously Updated)**

**Profile behaviors**

* System-created after Setup Course.  
* Continuously updated as user completes lessons/activities/assessments.  
* User can edit at any time (with change history).

**Minimum profile fields**

* `displayName`  
* `learningGoals[]`  
* `interests[]`  
* `experienceLevel` (per domain if possible)  
* `preferredLearningStyle` (text-heavy vs examples-heavy, etc.)  
* `udlPreferences` (Engagement / Representation / Action-Expression)  
* `constraints` (time per day, device constraints)  
* `badgeInventory[]`  
* `courseHistory[]` (high-level: started/completed)  
* `skillSignals[]` (derived: strengths, gaps, misconceptions)  
* `tonePreference` (optional)

**Acceptance criteria**

* User can view/edit profile and save.  
* System updates are visible (e.g., “Updated based on your last activity: …”).

---

### **4.4 Course Discovery**

Users can:

* Select from **predefined courses**  
* Create a **custom course** by entering:  
  * brief description  
  * learning objectives

**Acceptance criteria**

* Course catalog shows predefined courses from local JSON course folders.  
* “Create Course” flow stores user input and triggers agent pipeline.

---

### **4.5 Course Generation for Predefined Courses**

When selecting a predefined course:

* App loads `course.json` (name, description, learning objectives)  
* App combines it with Learner Profile signals  
* App generates personalized lessons/activities/assessment via agents

**Acceptance criteria**

* Course renders with left-nav structure after generation finishes (or progressively as lessons complete).

---

### **4.6 Course Page Structure**

**Layout**

* Main course page includes:  
  * Left-hand navigation:  
    * Lessons (ordered)  
    * Activities (paired after each lesson)  
    * Final assessment  
  * Main content area:  
    * Current lesson/activity  
    * Progress indicator

**Rules**

* Each lesson is followed by an activity.  
* Completing an activity:  
  * triggers personalized feedback  
  * unlocks the next lesson  
* Course completion requires finishing the assessment.

**Acceptance criteria**

* Navigation shows locked/unlocked state.  
* User cannot open locked lessons/activities unless override/admin mode exists.  
* Progress updates immediately after completion.

---

### **4.7 Activities and Feedback**

**Activity modalities**

* Short response (text)  
* Image upload (for diagrams, handwritten work, screenshots)

**Feedback**

* Personalized feedback is shown after activity submission  
* Feedback includes:  
  * what was correct  
  * what to improve  
  * next steps aligned to the learning objective  
  * optionally suggests revisiting a portion of the lesson

**Acceptance criteria**

* Submitting activity triggers activity reviewer and returns feedback \+ score \+ tips.  
* Unlocking logic runs only on successful submission (submission success ≠ mastery; mastery may be gated by rubric threshold if configured).

---

### **4.8 Assessment and Badge Award**

**Assessment**

* Generated once all lessons are completed.  
* Contains short response and/or image upload items validating all objectives.

**Completion**

* When assessment is completed, course is marked complete.  
* A special badge is added to user profile.

**Acceptance criteria**

* Course cannot be marked complete without assessment submission and review.  
* Badge appears in Learner Profile immediately on completion.

---

### **4.9 Agent Log**

**Purpose**  
Provide transparency: a summary of work done by each agent including:

* Prompt sent to the agent  
* Output returned by the agent  
* Metadata: timestamps, model/version, duration, status, token usage (if available)

**Scope**

* Visible per course, per lesson/activity/assessment.

**Acceptance criteria**

* Every agent run writes a log record.  
* Logs are viewable in UI with filtering by lesson/activity.  
* Sensitive data redaction rules are applied (see Security).

---

## **5\) Functional Requirements**

### **5.1 Course Types**

* **Predefined Course**  
  * Source: local `apps_directory/courses/<course_id>/course.json`  
* **User-created Course**  
  * Source: stored in DB, optional local cache  
  * Input: description \+ objectives

### **5.2 Course State Machine**

**States**

* Draft (created, not generated)  
* Generating  
* Active (lessons available)  
* In Progress  
* Awaiting Assessment Generation  
* Assessment Ready  
* Completed  
* Archived (optional)

### **5.3 Progress Tracking**

Track at minimum:

* Lesson viewed/completed  
* Activity submitted \+ score \+ feedback  
* Assessment submitted \+ score \+ feedback  
* Timestamps and attempt counts

### **5.4 Content Editing (v1 baseline)**

* User can:  
  * regenerate a lesson/activity (limited times)  
  * request “more examples” or “simpler explanation”  
* Admin/system can:  
  * update predefined course JSON and version it

---

## **6\) Data & Storage Requirements**

### **6.1 Local course JSON structure**

**File path**  
`/app/courses/<course_id>/course.json`

**Schema (v1)**

{  
  "courseId": "string",  
  "version": "string",  
  "name": "string",  
  "description": "string",  
  "learningObjectives": \[  
    "string"  
  \],  
  "tags": \["string"\],  
  "estimatedHours": "number"  
}

### **6.2 Database entities (minimum)**

**User**

* id, authProvider, email, createdAt, lastLoginAt

**LearnerProfile**

* userId (FK)  
* profileJson (versioned)  
* updatedAt  
* updateSource (setup\_course, activity\_signal, user\_edit)

**CourseInstance**

* id  
* userId  
* sourceType (predefined | user\_created)  
* sourceCourseId (if predefined)  
* inputDescription (if user created)  
* inputObjectives\[\]  
* generatedCourseDescription (agent output)  
* status  
* createdAt, updatedAt

**Lesson**

* id, courseInstanceId, objectiveIndex  
* lessonPlanJson  
* lessonContent (rich text / markdown)  
* visualAids\[\]  
* status (locked/unlocked/completed)

**Activity**

* id, lessonId  
* activitySpecJson  
* submission(s) including text/image pointers  
* reviewerScore, reviewerFeedback  
* status

**Assessment**

* id, courseInstanceId  
* assessmentSpecJson  
* submission(s)  
* reviewerScore, reviewerFeedback  
* status

**Badge**

* id, userId, courseInstanceId, badgeType, awardedAt

**AgentLog**

* id  
* userId  
* courseInstanceId  
* lessonId/activityId/assessmentId (nullable)  
* agentName  
* prompt  
* output  
* status (success/error)  
* timings, model metadata  
* redactionFlags

### **6.3 Security and privacy requirements**

* Encrypt sensitive fields at rest (profile, submissions if required).  
* Store image uploads in secure object storage with expiring signed URLs.  
* Token handling: secure storage (keychain/secure enclave on device where applicable).  
* Agent Log redaction:  
  * remove access tokens  
  * remove raw passwords  
  * optionally mask PII depending on policy (email, full name)  
* Provide “Delete my data” to remove user profile \+ generated content \+ uploads (retain minimal audit if legally required).

### **6.4 Local storage usage**

* Cache course JSON and generated artifacts locally when possible:  
  * last opened course instance  
  * unlocked lesson content  
* Must not store raw auth tokens in plain local storage.

---

## **7\) Agent System Requirements**

## **7.1 Agent Orchestration Principles**

* Agents are called in a deterministic pipeline with clear contracts.  
* Each agent call must:  
  * declare inputs and outputs in JSON  
  * validate output schema  
  * write AgentLog entry  
* Support retries and fallbacks:  
  * if agent fails schema validation, auto-reprompt once with error details  
  * if still failing, mark step as error and show user a recovery UI (regenerate, contact support)

## **7.2 Agent List and Contracts**

Naming note: Use these exact agent identifiers in logs and orchestration.

---

### **A) course\_describer (course mapping agent)**

**Purpose**  
Create a course description focused on a single objective, based on preloaded course info \+ learner profile.

**Inputs**

* course name, base description  
* learning objectives\[\]  
* learner profile  
* selected objective index or objective text

**Outputs**

{  
  "focusedObjective": "string",  
  "courseDescription": "string",  
  "personalizationRationale": \["string"\]  
}

**Rules**

* Description must be concise and specific.  
* Must align to the selected objective.

---

### **B) lesson_planner (lesson plan agent)**

**Purpose**

Generate a *single* lesson plan for *one* learning objective. The plan must be specific enough that downstream agents (lesson\_writer, activity\_creator, reviewers) can produce aligned instruction, practice, and assessment without guessing.

#### Input (to agent)

Provide the agent these inputs every run:

* courseDescription (string): Personalized course description (focused or general).  
* objectiveSource (string): The original learning objective text (from course JSON or user input).  
* learnerProfile (object): Key signals only (experience level, interests, UDL needs, preferred modality, constraints like time).  
* constraints (object, optional):  
  * timeMinutes (number)  
  * allowedSubmissionTypes (array: \["short\_response","image\_upload","either"\])  
  * readingLevel (e.g., “grade 8”, “professional”)

#### Output (from agent) — 

##### STRICT JSON

The agent must output **only** valid JSON matching this schema. No markdown, no commentary.

{

  "learningObjective": {

    "statement": "string",

    "measurableVerb": "string",

    "successEvidence": \["string"\]

  },

  "competency": {

    "index": 1,

    "label": "string",

    "summary": "string"

  },

  "enduringUnderstanding": {

    "whyItMatters": "string",

    "keyTakeaways": \["string"\]

  },

  "essentialQuestions": \["string", "string"\],

  "assessmentProject": {

    "artifactName": "string",

    "prompt": "string",

    "submissionType": "short\_response|image\_upload|either",

    "bloomsLevel": "remember|understand|apply|analyze|evaluate|create",

    "webbDOK": 1,

    "requirements": \["string"\],

    "scoringDimensions": \["string"\]

  },

  "masteryCriteria": {

    "successMetric": "string",

    "rubricChecks": \["string"\]

  },

  "udlAccommodations": {

    "engagement": \["string"\],

    "representation": \["string"\],

    "actionExpression": \["string"\]

  },

  "activities": \[

    {

      "title": "string",

      "type": "read|watch|practice|reflect|discuss|build",

      "instructions": "string",

      "estimatedMinutes": 10,

      "outputs": \["string"\],

      "alignment": {

        "objectiveEvidence": \["string"\],

        "rubricChecks": \["string"\]

      }

    }

  \]

}

#### Field-by-field requirements (clarified)

##### 1\) learningObjective

* **statement**: Restate the objective in **first person**, **one sentence**, measurable, and aligned to the assessment.  
  * Example pattern: “I can *\[measurable verb\]* *\[thing\]* *\[context\]* *\[quality/constraint\]*.”  
* **measurableVerb**: A single verb from Bloom’s-aligned verbs (e.g., “draft”, “compare”, “design”, “justify”).  
* **successEvidence**: 2–4 bullet items describing what observable proof looks like.

##### 2\) competency

* **index**: Integer used as the numbered heading (e.g., 1, 2, 3).  
* **label**: Short heading (3–8 words), e.g., “1. Prompting for Reliable Outputs”.  
* **summary**: One sentence describing the broader skill area.

##### 3\) enduringUnderstanding

* **whyItMatters**: 1–2 sentences on long-term value.  
* **keyTakeaways**: 2–4 concise bullets that the lesson should reinforce.

##### 4\) essentialQuestions

* Exactly **2–4** questions.  
* Must be answerable by the end of the lesson \+ assessment project.  
* Avoid yes/no questions; prefer “how/why/what” framing.

##### 5\) assessmentProject

This is the **single portfolio artifact** proving the objective.

* **artifactName**: Clear name (e.g., “One-Page Strategy Memo”).  
* **prompt**: The exact instructions a learner sees.  
* **submissionType**: One of short\_response, image\_upload, either.  
* **bloomsLevel**: One of the enumerated Bloom levels.  
* **webbDOK**: Integer 1–4.  
* **requirements**: 3–7 concrete requirements (length, components, constraints).  
* **scoringDimensions**: 3–6 dimensions (e.g., “accuracy”, “clarity”, “justification”, “completeness”).

##### 6\) masteryCriteria

* **successMetric**: A single sentence defining what “meets mastery” means.  
* **rubricChecks**: **3–6** rubric-style checks.  
  * Must be binary-checkable where possible (e.g., “Includes 3 stakeholder needs with evidence.”)  
  * Must map directly to assessment requirements/scoring dimensions.

##### 7\) udlAccommodations

Each array must contain **2–4** accommodations tailored to:

* the objective  
* the assessment format  
* learner profile needs

Categories:

* **engagement**: choice, relevance, motivation, scaffolding  
* **representation**: multiple formats, examples, vocabulary support  
* **actionExpression**: multiple ways to respond, tools, templates, assistive tech

##### 8\) activities

Provide **3–6** activities that prepare learners to succeed on the assessment.

Each activity must include:

* **title**: short and specific  
* **type**: one of read|watch|practice|reflect|discuss|build  
* **instructions**: actionable steps (not vague)  
* **estimatedMinutes**: numeric estimate  
* **outputs**: what the learner produces (notes, draft, checklist, etc.)  
* **alignment**:  
  * **objectiveEvidence**: which evidence items this activity supports  
  * **rubricChecks**: which rubric checks it prepares for (must match strings used in masteryCriteria.rubricChecks)

#### Global validation rules (must follow)

1. Output must be **JSON only**, no extra keys.  
2. Essential questions: **2–4**.  
3. Rubric checks: **3–6**.  
4. Activities: **3–6**.  
5. Assessment project must produce **one artifact** only.  
6. Every activity must align to at least:  
   * one learningObjective.successEvidence item, and  
   * one masteryCriteria.rubricChecks item.  
7. Use learner profile to:  
   * adapt examples/context,  
   * set appropriate difficulty,  
   * include relevant UDL accommodations.

#### Short “definition of done”

A lesson plan is acceptable if a downstream lesson writer can produce the lesson and activity *without inventing missing details*, and the assessment rubric can be applied objectively to determine mastery.

---

### **C) lesson\_writer**

**Purpose**  
Write full lesson content and suggest an activity.

**Inputs**

* lesson plan JSON  
* focused course description  
* learner profile

**Outputs**

{  
  "lessonTitle": "string",  
  "lessonBody": "string",  
  "keyTakeaways": \["string"\],  
  "suggestedActivity": {  
    "type": "short\_response|image\_upload|either",  
    "prompt": "string",  
    "expectedEvidence": \["string"\]  
  }  
}

**Rules**

* Lesson body should be structured with headings, steps, and examples.  
* Must implement UDL accommodations where feasible (formatting, multiple representations).

---

### **D) activity\_creator**

**Purpose**  
Convert suggested activity into a custom activity that tests comprehension.

**Inputs**

* suggested activity  
* lesson plan (objective \+ mastery criteria)  
* learner profile

**Outputs**

{  
  "activityId": "string",  
  "activityType": "short\_response|image\_upload|either",  
  "instructions": "string",  
  "prompt": "string",  
  "submissionFormat": {  
    "text": true,  
    "image": false  
  },  
  "scoringRubric": \["string"\],  
  "hints": \["string"\]  
}

**Rules**

* Rubric must map to mastery criteria.  
* If image upload is allowed, specify what the image should show.

---

### **E) activity\_reviewer**

**Purpose**  
Review the activity submission and score it based on ability to achieve objective.

**Inputs**

* objective (first person)  
* mastery criteria  
* activity prompt \+ rubric  
* user submission (text and/or image metadata \+ link)  
* learner profile (optional)

**Outputs**

{  
  "score": 0,  
  "maxScore": 100,  
  "rationale": "string",  
  "strengths": \["string"\],  
  "improvements": \["string"\],  
  "tips": \["string"\],  
  "masteryDecision": "not\_yet|meets|exceeds"  
}

**Rules**

* Must be constructive and specific.  
* Must reference rubric items.

---

### **F) visual\_aid\_sniffer**

**Purpose**  
Determine if informative graphics are needed.

**Inputs**

* lesson body  
* UDL accommodations  
* learner profile

**Outputs**

{  
  "needsVisualAids": "yes|no",  
  "visualAids": \[  
    {  
      "title": "string",  
      "description": "string",  
      "placementHint": "string"  
    }  
  \]  
}

**Rules**

* If “yes”, must include 1+ visual aid specs and where to place them.

---

### **G) visual\_aid\_creator**

**Purpose**  
Generate each visual aid and alt text.

**Inputs**

* visual aid spec (title/description/placement)  
* lesson context (optional)  
* learner profile accessibility needs

**Outputs**

{  
  "assetType": "svg|png|mermaid",  
  "asset": "string",  
  "altText": "string"  
}

**Rules**

* Prefer SVG or Mermaid for accessibility and responsiveness.  
* Alt text must be meaningful, not decorative.

---

### **H) assessment\_creator**

**Purpose**  
Create assessment after all lessons complete.

**Inputs**

* course description  
* learning objectives\[\]  
* learner profile  
* activity reviewer outputs across lessons (scores, gaps, tips)

**Outputs**

{  
  "assessmentTitle": "string",  
  "items": \[  
    {  
      "objective": "string",  
      "type": "short\_response|image\_upload|either",  
      "prompt": "string",  
      "rubric": \["string"\]  
    }  
  \],  
  "submissionRules": {  
    "allowText": true,  
    "allowImage": true  
  }  
}

**Rules**

* Must cover all learning objectives.  
* Keep it short; prioritize evidence of mastery.

---

### **I) assessment\_reviewer**

**Purpose**  
Review the assessment submission and score objective mastery.

**Inputs**

* assessment spec \+ rubrics  
* submission(s)  
* learner profile

**Outputs**

{  
  "overallScore": 0,  
  "maxScore": 100,  
  "objectiveScores": \[  
    {  
      "objective": "string",  
      "score": 0,  
      "maxScore": 100,  
      "feedback": "string"  
    }  
  \],  
  "passDecision": "fail|pass",  
  "nextSteps": \["string"\]  
}

**Rules**

* Must provide actionable next steps per weak objective.

---

## **8\) Orchestration Flows**

### **8.1 Predefined course generation (happy path)**

1. Load course JSON (name, description, objectives).  
2. For each objective:  
   * `course_describer` (focused objective)  
   * `lesson_planner`  
   * `lesson_writer`  
   * `visual_aid_sniffer`  
   * if yes: run `visual_aid_creator` per aid  
   * create `activity_creator`  
3. Unlock Lesson 1 by default; others locked.  
4. After each activity submission:  
   * `activity_reviewer` → feedback → unlock next lesson  
5. After all lessons complete:  
   * `assessment_creator` → assessment ready  
6. After assessment submission:  
   * `assessment_reviewer` → if pass: mark complete \+ award badge

### **8.2 User-created course generation**

Same pipeline, but base course JSON is replaced by user description \+ objectives.

---

## **9\) UX Screens (Minimum Set)**

1. **Auth Screen** (SSO \+ create account)  
2. **Setup Course Landing**  
3. **Lesson Viewer**  
4. **Activity Submission**  
5. **Feedback Screen** (post-activity)  
6. **Course Overview** (left-nav, progress)  
7. **Assessment Screen**  
8. **Learner Profile** (view/edit \+ badges)  
9. **Course Catalog** (predefined \+ create course)  
10. **Agent Log Viewer** (filters, detail modal)

---

## **10\) Error Handling & Recovery**

### **Agent failure**

* If an agent fails:  
  * show step-level error (“Lesson 2 generation failed”)  
  * allow “Retry generation” and “Use fallback version”  
* Fallback version:  
  * simplified prompt template  
  * reduced personalization

### **Submission issues**

* Upload fail: retry, size limits, supported formats.  
* Reviewer fail: show “pending review” and run retry.

---

## **11\) Accessibility (Core Requirements)**

* All content must be screen-reader friendly (semantic headings, lists, landmarks).  
* Visual aids require meaningful alt text.  
* Support keyboard navigation for left-nav and activity forms.  
* Provide adjustable reading mode (font size, spacing).  
* Ensure UDL accommodations are surfaced:  
  * multiple representations (examples, summaries)  
  * multiple response options (text/image)  
  * engagement hooks based on interests

---

## **12\) Analytics & Telemetry (Privacy-aware)**

Track events (no raw submissions unless explicitly opted-in):

* signup/login  
* setup\_started/completed  
* course\_selected/course\_created  
* lesson\_viewed  
* activity\_submitted \+ score bucket  
* assessment\_submitted \+ pass/fail  
* regeneration\_requested  
* agent\_failure \+ agentName

---

## **13\) Acceptance Criteria Checklist (v1)**

### **Core**

* Users can authenticate (SSO or new account).  
* Setup Course required on first run; generates learner profile.  
* Learner profile is editable and continuously updated.  
* Predefined course selection triggers personalized lesson generation.  
* Course left-nav shows lessons, activities, assessment with lock/unlock.  
* Each lesson followed by an activity.  
* Activity submission produces feedback and unlocks next lesson.  
* Progress is tracked and displayed.  
* Assessment generated after lessons complete.  
* Assessment completion awards badge and marks course complete.  
* Agent Log shows prompt \+ output per agent run.

### **Data & Security**

* Data stored securely in DB; local caching where appropriate.  
* Course JSON loaded from course folder in app directory.  
* Logs are redacted for secrets/PII per policy.

---

## **14\) Implementation Notes (LLM/Dev-Oriented)**

### **Content format**

* Use Markdown for lesson bodies (rendered to rich text in UI).  
* Visual aids: prefer Mermaid/SVG stored as text assets for accessibility.

### **Schema validation**

* Every agent output must be validated against JSON schema.  
* Store both raw output and parsed/validated output.

### **Determinism controls**

* Use temperature controls and system prompts per agent to reduce drift.  
* Keep agent prompts versioned so outputs are reproducible.

---

## **15\) Open Items (Defined but Not Fully Specified)**

* Badge taxonomy (types, designs, criteria).  
* Objective selection strategy for `course_describer` (one objective at a time vs a synthesized “primary objective”).  
* Mastery gating thresholds (e.g., unlock if submitted vs unlock if meets masteryDecision).  
* Image rubric evaluation approach (human-in-the-loop vs model vision).
